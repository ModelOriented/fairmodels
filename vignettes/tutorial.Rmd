---
title: "tutorial"
author: "Jakub WiÅ›niewski"
output: 
  html_document: 
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true      
vignette: >
  %\VignetteIndexEntry{tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  
---
# Objects and Structure
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)

```

```{r setup, include=FALSE}
library(FairModels)

library(DALEX)
library(ranger)
library(gbm)

```
## Why? 

Let's say you are building court system that predicts if someone will become recidivist in the futre. First you gather information
then you build a model and predict outcomes. You get accuracy score of 90%. It is preety good, but it appears that the model is more likely to say that African Americans will become recidivists. Model was trained on data that was discriminating certain ethnic groups. So now we have some options. First one is to change the data, and the second one is to tune model, and check if it behaves as we would like it to be. We will choose the second option.

## Data

We will use modified ProPublica's compass data to represent our problem. 

```{r}
data("compas")

head(compas)
```

We train and explain our model with `DALEX`
```{r}
# Train
rf_compas <- ranger(Two_yr_Recidivism ~., data = compas, probability = TRUE) # Wszystko

# numeric target values
y_numeric <- as.numeric(compas$Two_yr_Recidivism)-1

# explainer
rf_explainer <- explain(rf_compas, data = compas[,-1], y = y_numeric)
```

Than we create `fairness_object` with `create_fairness_object()`

```{r}

fobject <- create_fairness_object(rf_explainer,
                                  data = compas, # it is important to specify all data if explainer doesn't have it
                                  outcome = "Two_yr_Recidivism", # target variable
                                  group  = "Ethnicity",          # protected group
                                  base   = "Caucasian")          # to what subgroupwe want to compare
                                                                 # cutoff - optional, deafult = 0.5
```

And we plot!
```{r}
plot(fobject)
```
   
As we can see it is more likely that model will categorise African Americans as future recidivists than for example Asians. 
But mabey some groups are statisticaly more likely to go do crimes in the future. It is possible, but there are more ways that the model can discriminate people. 

## Fairness Object

To really see what `fairness_object` is about, we need to make some more models and explainers. 

```{r, results= "hide"}
set.seed(123)

rf_compas_1 <- ranger(Two_yr_Recidivism ~Number_of_Priors+Age_Below_TwentyFive, data = compas, probability = TRUE)
lr_compas_1 <- glm(Two_yr_Recidivism~., data=compas, family=binomial(link="logit"))
rf_compas_2 <- ranger(Two_yr_Recidivism ~., data = compas, probability = TRUE) 
rf_compas_3 <- ranger(Two_yr_Recidivism ~ Age_Above_FourtyFive+Misdemeanor, data = compas, probability = TRUE)
rf_compas_4 <- ranger(Two_yr_Recidivism ~., data = compas, probability = TRUE)
df <- compas
df$Two_yr_Recidivism <- as.numeric(compas$Two_yr_Recidivism)-1
gbm_compas_1<- gbm(Two_yr_Recidivism~., data = df) 

explainer_1 <- explain(rf_compas_1,  data = compas[,-1], y = y_numeric)
explainer_2 <- explain(lr_compas_1,  data = compas[,-1], y = y_numeric)
explainer_3 <- explain(rf_compas_2,  data = compas[,-1], y = y_numeric, label = "ranger_2")
explainer_4 <- explain(rf_compas_3,  data = compas[,-1], y = y_numeric, label = "ranger_3")
explainer_5 <- explain(gbm_compas_1, data = compas[,-1], y = y_numeric)
explainer_6 <- explain(rf_compas_4,  data = compas[,-1], y = y_numeric, label = "ranger_4")

```
   
Now we create one object with all explainers
```{r}
fobject <- create_fairness_object(explainer_1,explainer_2,explainer_3,explainer_4,explainer_5,explainer_6,
                                  data = compas, 
                                  outcome = "Two_yr_Recidivism",
                                  group  = "Ethnicity",
                                  base   = "Caucasian") 
```
   
As we can see there is some parameters in fobject such as:    
1. DALEX  Explainer or list of explainers   
2. data - if first explainer was not provided with whole data frame, we should do it here    
3. outcome - target variable, what we want to predict   
4. group - protected group/ sensitive variable which can be unfairly treated   
5. base - to what subgroup we want to compare metric scores with. If not provided gets first subgroup in data.   

What consists of fairness object? 
```{r}
print(fobject)
```

Fairness object gets metrics based on confussion matrix and checks them over the groups. 
```{r}
# for the first model
fobject$groups_data[[1]]$fpr_parity
```

If we were going only to take score from certain metric (Let's say fpr and 0.3) we wouldn't know if it is good or bad. But we are aiming for **equal treatment over all groups** so if this metric score would be the same in all groups it would be very good. But the metrics wouldn't be comparable between each others (fpr - 0.3 in all groups and acc_parity - 0.9 in all groups, both are good in terms of parity). That is wy we use **base** - to set benchmark. And for example Caucasian in fpr had score of 0.3 and African American 0.6. After setting `base = Caucasian` Caucasian would have score 1, and African American 2, because is two times higher score in fpr.

*Note: When dealing with aggregating plots we use formula sum(abs(1-score)) to represent aggregated score in metrics. In short is how much it differs from ideal scores.* 


# Choosing best model

We now have a few models in our `fairness_object`

Let's see how they perform in diffrent metrics.

## Stacked Barplot
```{r}
plot_stacked_barplot(fobject)
```
   
At first we see that lm is the worst overall model. It does not give us information if plots are similar to each other. 

## Plot fairness PCA
With this task we should use `PCA`. We call `create_fairness_pca()` to create fairness pca object. 
```{r}
fair_pca <- create_fairness_pca(fobject)
print(fair_pca)
```

Let's plot!
```{r}
plot(fair_pca)
```

It is done with loadings plot, which can be customised. 
```{r}
plot(fair_pca, scale = 0) # deafult = 0.5
```

## Plot Heatmap
Another way to deal with grouped data is using heatmap.

```{r}
plot_heatmap(fobject, subtitle = "With cool colors and dendograms")
```
   
For both models and metrics dendograms are created. This way through hierarchical clustering we can look on similarities between models/metrics. It should give similar but more detailed information than PCA
   
It can be normalised among metrics.
```{r}
plot_heatmap(fobject, scale = TRUE, title = "Title can be changed") # we can turn values off via text= FALSE
```

Now we know what those scores are and how "similar" models are to each other
## Metric and Performance Plot

Sometimes we would like to know how good are models in performance metrics and in fairness metrics at the same time, to see the tradeoff between them.

```{r}
plot_performance_with_fairness(fobject, fairness_metric = "fpr_parity")
```

We can add plots with help of `patchwork` 

```{r, results= "hide"}
library(patchwork)
library(ggplot2)

p1 <- plot_performance_with_fairness(fobject,  "equal_odds", "accuracy")
p2 <- plot_performance_with_fairness(fobject,  "fpr_parity", "auc")     + ggtitle(" ") + theme(legend.position = "none")
p3 <- plot_performance_with_fairness(fobject,  "npv_parity", "recall")  + ggtitle(" ") + theme(legend.position = "none")
p4 <- plot_performance_with_fairness(fobject,  "pred_rate_parity", "f1")+ ggtitle(" ") + theme(legend.position = "none")

```
```{r}
(p1 + p2)/(p3+p4)
```



## Back to detail 

When we have narrowed down our search for the best model we can use `two_models_plot` to check once again metrics within groups and decide which model to use.

```{r}
fobject2 <- create_fairness_object(explainer_1,explainer_3, 
                                   data = compas, 
                                  outcome = "Two_yr_Recidivism",
                                  group  = "Ethnicity",
                                  base   = "Caucasian")

plot_two_models(fobject2, fairness_metric = "fpr_parity") # both fairness_metric and performance_metric have deafult values
```

We can also use simple `plot` when we have only few explainers.
```{r}
plot(fobject2)
```

Of course the protected group can be changed.
```{r}
fobject3 <- create_fairness_object(explainer_2,explainer_5, 
                                   data = compas, 
                                  outcome = "Two_yr_Recidivism",
                                  group  = "Sex",
                                  base = "Female")  
plot(fobject3)
```


